{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# 6. CRP + Hawkes process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports \n",
    "from ast import literal_eval\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data\n",
    "\n",
    "First, we need to load the data from the csv. This file contains the documents already processed and cleaned after applying the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization, which includes:\n",
    "    1. Removing capitalization.\n",
    "    2. Removing non alphanumeric tokens (e.g. punktuation signs)\n",
    "    3. Stemming/Lemmatization.\n",
    "3. Cleaning\n",
    "4. Vectorization\n",
    "\n",
    "\n",
    "We load it as a `pandas` dataframe. \n",
    "\n",
    "***Note***: The timestamp column contains the time at which the event was generated in ***seconds***.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tweets_cleaned.csv')\n",
    "df.drop_duplicates(subset=\"tweet\", inplace=True)\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(literal_eval) #Transform the string into a list of tokens\n",
    "df.sort_values(by='timestamp', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionary\n",
    "\n",
    "Up to this point, we have transformed the raw text collection in a list of documents stored in `X_tokens`, where each document is a collection \n",
    "of the words that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into \n",
    "a numerical representation (a list of vectors, or a matrix). To do so, we will start using the tools provided by the `gensim` library. \n",
    "\n",
    "As a first step, we create a dictionary containing all tokens in our text corpus, and assigning an integer identifier to each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = 350\n",
    "X_tokens = list(df['tokens'].values)\n",
    "dictionary = gensim.corpora.Dictionary(X_tokens)\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=I)\n",
    "\n",
    "I = len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset for the HDHP\n",
    "\n",
    "Build the dataset `X_events` as a list of events where each event is a tuple `X_events[i] = (ts, w, user_id, meta_info)` of four values:\n",
    "- `ts`: timestamp of the event. It is important to choose the base time scale (e.g., days, months) and it is also recommended to normalized them so that the timestamp of the first event is 0.\n",
    "- `w`: represents the content of the document. It should be a string containing the tokens separated by a white space.\n",
    "- `user_id`: the id of the user that generated the event. They should go from 0 to the number of users contained in the dataset\n",
    "- `meta_info`: list with extra information. It is not needed for the tutorial so it can be set to `[]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 2.1:***  Implement the code to build the X_event list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_events = list()\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Dirichlet Hawkes Proces (HDHP)\n",
    "\n",
    "Import the Hierarchical Dirichlet Hawkes Proces module. Complete details about the code can be found in the official Github repository at https://github.com/Networks-Learning/hdhp.py. There, you can also find an example of how to use the package in the Jupyter notebooks under the directory examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 1.1***: Find the piece of code  where the $\\mu_u$ parameters are updated.  If you cannot find it, explain the reason.\n",
    "\n",
    "TODO\n",
    "\n",
    "***Exercise 1.2***: Find the piece of code where the $\\alpha_l$ parameters are updated.  If you cannot find it, explain the reason.\n",
    "\n",
    "TODO\n",
    "\n",
    "***Exercise 1.3***:  Find the piece of code where the $\\theta_l$ parameters are updated. If you cannot find it, explain the reason.\n",
    "\n",
    "TODO\n",
    "\n",
    "***Exercise 1.4***: Find the piece of code where the $z_{1:n}$ table's assignment variables are updated.  If you cannot find it, explain the reason.\n",
    "\n",
    "TODO\n",
    "\n",
    "***Exercise 1.5***: Explore the code and explain how the final particle is chosen at the last iteration. Recall, we run the SMC with $|P|$ particles but at the end we only consider one sample per parameter to show the results.\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdhp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(dictionary.values())\n",
    "doc_min_length = 2\n",
    "doc_length = 10\n",
    "words_per_pattern = 15\n",
    "\n",
    "alpha_0 = (2.5, 0.75)  # Parameters of the prior for the alpha, i.e,, Gamma distribution\n",
    "mu_0 = (2, 0.5)  # Parameters of the prior for the mu_u, i.e, Gamma distribution\n",
    "omega = 1  # Decay of the exponential  kernel, i.e., memory of the intensity.\n",
    "beta = 0.01 # Concentration parameter for the Dirichlet Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "particle, norms = hdhp.infer(X_events, alpha_0=alpha_0, mu_0=mu_0, omega=omega, num_particles=15, beta=beta, seed=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 2.2***: Compute the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 2.3***: From the HDHP code provided extract the necesary information about the learning patterns to fill the requested table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 2.4***:  From the HDHP code provided extract the necesary information about the users to fill the requested table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
