{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.9 |Anaconda custom (64-bit)| (default, Jul 30 2019, 13:42:17) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "# Common imports \n",
    "from ast import literal_eval\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from categorical_em import CategoricalEM\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## 1. Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "K = 5 # Number of mixture components\n",
    "I = 120 # Number of words in the dictionary\n",
    "N = None # Number of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the data\n",
    "\n",
    "First, we need to load the data from the csv. This file contains the documents already processed and cleaned after applying the following steps:\n",
    "\n",
    "1. Tokenization\n",
    "2. Homogeneization, which includes:\n",
    "    1. Removing capitalization.\n",
    "    2. Removing non alphanumeric tokens (e.g. punktuation signs)\n",
    "    3. Stemming/Lemmatization.\n",
    "3. Cleaning\n",
    "4. Vectorization\n",
    "\n",
    "\n",
    "We load it as a `pandas` dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets_cleaned.csv')\n",
    "df.drop_duplicates(subset=\"tweet\", inplace=True)\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(literal_eval) #Transform the string into a list of tokens\n",
    "X_tokens = list(df['tokens'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: tweet_id | timestamp | user_id | tweet | tweets_clean | tokens\n",
      "\n",
      "Tweet:\n",
      "OSINT people - please retweet, if possible. My friend is looking for women involved in OSINT. https://twitter.com/manisha_bot/status/1181594280336531457 …\n",
      "Tweet cleaned:\n",
      "osint people   please retweet  if possible  my friend is looking for women involved in osint\n",
      "Tweet tokens:\n",
      "['osint', 'peopl', 'retweet', 'possibl', 'friend', 'look', 'woman', 'involv', 'osint']\n"
     ]
    }
   ],
   "source": [
    "print('Columns: {}\\n'.format(' | '.join(df.columns.values)))\n",
    "\n",
    "print('Tweet:\\n{}'.format(df.loc[1, 'tweet']))\n",
    "print('Tweet cleaned:\\n{}'.format(df.loc[1, 'tweets_clean']))\n",
    "print('Tweet tokens:\\n{}'.format(X_tokens[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionary\n",
    "\n",
    "Up to this point, we have transformed the raw text collection in a list of documents stored in `X_tokens`, where each document is a collection \n",
    "of the words that are most relevant for semantic analysis. Now, we need to convert these data (a list of token lists) into \n",
    "a numerical representation (a list of vectors, or a matrix). To do so, we will start using the tools provided by the `gensim` library. \n",
    "\n",
    "As a first step, we create a dictionary containing all tokens in our text corpus, and assigning an integer identifier to each one of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cf7f0046ff1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_extremes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_below\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_above\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(X_tokens)\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=I)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words (BoW): Numerical version of documents\n",
    "In the second step, let us create a numerical version of our corpus using the `doc2bow` method. In general, \n",
    "`D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in \n",
    "`token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences \n",
    "of such token in `token_list`. \n",
    "\n",
    "*Exercise:* Apply the `doc2bow` method from gensim dictionary `D`, to all tokens in every document in `X_tokens`. \n",
    "The result must be a new list named `X_bow` where each element is a list of tuples `(token_id, number_of_occurrences)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0680920d3af8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mkeep_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtweet_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_bow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mX_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "X_bow = list()\n",
    "keep_tweet = list()\n",
    "for tweet in X_tokens:\n",
    "    tweet_bow = dictionary.doc2bow(tweet)\n",
    "    if len(tweet_bow) > 1:\n",
    "        X_bow.append(tweet_bow)\n",
    "        keep_tweet.append(True)\n",
    "    else:\n",
    "        keep_tweet.append(False)\n",
    "\n",
    "df_data = df[keep_tweet]\n",
    "\n",
    "N = len(df_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the BoW representation `X_bow` into a matrix, namely `X_matrix`, in which the i-th row and j-th column represents the \n",
    "number of occurrences of the j-th word of the dictionary in the i-th document. This will be the matrix used in the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_matrix = np.zeros([N, I])\n",
    "for i, doc_bow in enumerate(X_bow):\n",
    "    word_list = list()\n",
    "    for word in doc_bow:\n",
    "        X_matrix[i, word[0]] = word[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Categorical Mixture Model with Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Analytical forms of the E and M steps for the EM-Algorithm\n",
    "1. Write the joint distribution: $p(\\{\\mathbf{x}_n, z_n\\}| \\Theta) = ?$\n",
    "2. Write the analytical expression for $Q(\\Theta, \\Theta^{\\text{old}}) = ?$\n",
    "3. Write the MLE for $\\Theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1\n",
    "\n",
    "\\begin{align}\n",
    "p(\\{\\mathbf{x}_n, z_n\\}| \\Theta) =  \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2\n",
    "\n",
    "\\begin{align}\n",
    "Q(\\Theta, \\Theta^{\\text{old}}) = \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.3\n",
    "\\begin{align}\n",
    "\\hat{\\pi}_k = \\cdots\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta}_{km} = \\cdots\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Data anlysis task\n",
    "#### Exercise 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER: 0 Q= -113938.6894 diff= 200\n",
      "ITER: 5 Q= -106270.54 diff= 874.9335\n",
      "ITER: 10 Q= -103845.55 diff= 229.4907\n",
      "ITER: 15 Q= -103360.947 diff= 62.7847\n",
      "ITER: 20 Q= -103129.4652 diff= 43.7452\n",
      "ITER: 25 Q= -102945.6175 diff= 25.4653\n",
      "ITER: 30 Q= -102867.4741 diff= 11.5916\n",
      "ITER: 35 Q= -102823.8343 diff= 6.9968\n",
      "ITER: 40 Q= -102793.0763 diff= 6.5606\n",
      "ITER: 45 Q= -102749.1686 diff= 10.122\n",
      "ITER: 50 Q= -102690.746 diff= 9.0267\n",
      "ITER: 55 Q= -102655.0962 diff= 6.1814\n",
      "ITER: 60 Q= -102625.4566 diff= 6.262\n",
      "ITER: 65 Q= -102590.2887 diff= 7.2504\n",
      "ITER: 70 Q= -102554.5986 diff= 7.013\n",
      "ITER: 75 Q= -102523.1185 diff= 5.6888\n",
      "ITER: 80 Q= -102497.527 diff= 4.9639\n",
      "ITER: 85 Q= -102469.6639 diff= 5.8711\n",
      "ITER: 90 Q= -102443.7946 diff= 4.6469\n",
      "ITER: 95 Q= -102423.6168 diff= 3.6374\n",
      "ITER: 100 Q= -102407.3007 diff= 3.0841\n",
      "ITER: 105 Q= -102393.0427 diff= 2.7363\n",
      "ITER: 110 Q= -102379.6224 diff= 2.6735\n",
      "ITER: 115 Q= -102366.1641 diff= 2.7039\n",
      "ITER: 120 Q= -102352.8545 diff= 2.5975\n",
      "ITER: 125 Q= -102341.0105 diff= 2.176\n",
      "ITER: 130 Q= -102331.9069 diff= 1.579\n",
      "ITER: 135 Q= -102325.6581 diff= 1.0497\n",
      "ITER: 140 Q= -102321.5815 diff= 0.6803\n",
      "ITER: 145 Q= -102318.9258 diff= 0.4464\n",
      "ITER: 150 Q= -102317.1604 diff= 0.2997\n",
      "ITER: 155 Q= -102315.9632 diff= 0.2045\n",
      "ITER: 160 Q= -102315.1424 diff= 0.1406\n",
      "ITER: 165 Q= -102314.5768 diff= 0.097\n",
      "ITER: 170 Q= -102314.1854 diff= 0.0673\n",
      "ITER: 175 Q= -102313.9128 diff= 0.047\n",
      "ITER: 180 Q= -102313.723 diff= 0.0325\n",
      "ITER: 185 Q= -102313.5966 diff= 0.0206\n",
      "ITER: 190 Q= -102313.5316 diff= 0.0075\n"
     ]
    }
   ],
   "source": [
    "K = 5 # Number of mixture components\n",
    "i_theta = 5 # Dirichlet parameter from which the parameter is sampled for initialization\n",
    "i_pi = 0 # Dirichlet parameter from which the parameter is sampled for initialization\n",
    "\n",
    "model = CategoricalEM(K, I, N, delta=0.01, epochs=200, init_params={'theta': i_theta, 'pi': i_pi})\n",
    "model.fit(X_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def AIC():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3\n",
    "\n",
    "Some useful packages:\n",
    "- matplotlib https://matplotlib.org/\n",
    "- seaborn https://github.com/mwaskom/seaborn\n",
    "- wordcloud https://github.com/amueller/word_cloud\n",
    "- probvis https://github.com/psanch21/prob-visualize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_array = np.array(df_data['tweet'].values)\n",
    "\n",
    "# Show the 10 most representative words for each topic using a cloud of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the 10 most relevant documents for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the evolution of Q over the epochs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
