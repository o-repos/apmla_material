{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%mathplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: inverse SK model\n",
    "\n",
    "Consider the _inverse_ SK model: given a set of $M$ data samples $\\mathbf{s}_{1},\\dots,\\mathbf{s}_{M}$, estimate the values of $\\mathbf{J}$ and $\\mathbf{h}$ that better explains the observed data, where each sample is drawn forma a Boltzmann distribution with Hamiltonian:\n",
    "\n",
    "$$\n",
    "H(\\mathbf{s})=-\\sum_{i\\neq j}J_{ij}s_{i}s_{j} -\\sum_{i}h_{i}s_{i} \\quad.\n",
    "$$\n",
    "\n",
    "**Objective**: _sample_ configurations of $ N $ variables from the corresponding Boltzmann distribution for a particular realization of the couplings and compare the empirical mean of the magnetizations with the one inferred using TAP and MF.  \n",
    " For sampling, we use the Monte-Carlo-Markov-Chain (MCMC)  Metropolis-Hastings algorithm as we learned for the Curie Weiss model in tutorial 7. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point a) \n",
    "\n",
    "Derive equations for estimating the parameters $h_{i}^{TAP}$ and $J_{ij}^{TAP}$ analogous to what we have done in the lectures for MF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this solution sheet we will assume that $J_{ii} = 0 \\, \\forall i=1, \\ldots, N$, i.e. when writing sums like $\\sum_{j=1}^{N} J_{ij} s_j$ we will equivalently mean $\\sum_{j \\neq i} J_{ij} s_j$.\n",
    "\n",
    "Similarly, we will again consider the linear response theorem:\n",
    "$$ C_{ij} \\, := \\, <s_i s_j>_D - <s_i>_D <s_j>_D \\,= \\, \\frac{\\partial <s_i>_D}{\\partial h_j} $$ \n",
    "we obtain the following \n",
    "\n",
    "The solution for $\\mathbf{J}$ is equivalent to the following matrix equation:\n",
    "\n",
    "$$ C = \\beta P \\left( \n",
    "        1 + J \\, C - \\beta S \\, C  + 2 \\beta M \\, C\n",
    "    \\right)  \n",
    "$$\n",
    "with $P$ and $S$ diagonal with values\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P_{ii} &= 1 - m_i^2  \\\\\n",
    "S_{ii} &= \\sum_k J_{ik}^2 \\, (1-m_k^2)\n",
    "\\end{align}\n",
    "$$\n",
    "and $M$ defined as \n",
    "$$ M_{ik} = m_i J_{ik}^2 m_k  \\, . $$\n",
    "\n",
    "\n",
    "**COMMENTS**: consider only the case $i \\neq j$?\n",
    "\n",
    "**OBSERVATION**: throughout the following exercises we assume $\\beta = 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points b), c) and d)\n",
    "\n",
    " - b) Write a _jupyter_ notebook to test TAP and MF in this inference task.  Throughout the following exercises we assume $\\beta = 1$.\n",
    "     - i) Extract a `ground-truth' set of parameters $h_{i}^{GT} \\sim \\mathcal{N}(0,0.01)$ and $J_{ij}^{GT}\\sim \\mathcal{N}(0,J_{0}/N)$ with $J_{0}=1.$\n",
    "     - ii) Generate $M=1000$ samples of a system of $N=20,50, 100,1000$ random variables extracted using the above ground-truth.\n",
    "     - iii) Infer $\\mathbf{h}^{model}$,$\\mathbf{J}^{model}$ for model being MF and TAP using the equations derived in the lectures and in (a).\n",
    " - c)  Repeat this for other values of $J_{0}=0.1,10$, i.e. tuning the coupling strength.\n",
    " - d) Generate scatter plots with ${J}^{model}_{ij}$ vs $J_{ij}^{GT}$, similar for ${h}^{model}_{i}$ vs $h_{i}^{GT}$ (one plot for each of the 3 values of $J_{0}$). Plot all the different realizations in $N$ on the same scatter plot, distinguishing them by marker type. At the end we have a total of 12 plots (one per model, one per parameter $J$ or $h$, one per value of $J_{0}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_instance(N, J0):\n",
    "    \"\"\" sample h, J accordingly to model \n",
    "    \"\"\"\n",
    "    # FILL\n",
    "    return h, J\n",
    "\n",
    "def SK_simul(h, J, sample_size, sample_distance=100, t_eq=0):\n",
    "    \"\"\"MCMC sampling from Boltzmann distribution defined by h, J. \n",
    "    \n",
    "    Arguments:\n",
    "        h, J: array, matrix that define the Boltzmann distribution.\n",
    "        sample_size: int. Desired number of samples returned\n",
    "        sample_distance: int. Number of MCMC steps to perform between two samples\n",
    "        t_eq: int. Burn-in period length, i.e. number of MCMC steps performed before \n",
    "              the first sample is returned.\n",
    "    \"\"\"\n",
    "    N = h.shape[0]\n",
    "    S = np.random.choice([+1, -1], N)       \n",
    "    delta_E = # FILL\n",
    "    \n",
    "    def _MCMC_step(S):\n",
    "        k, r = np.random.randint(0, N), np.random.uniform(0, 1)\n",
    "        # FILL\n",
    "        \n",
    "    # burn-in steps\n",
    "    for _ in range(t_eq):\n",
    "        _MCMC_step(S)\n",
    "    # sampling\n",
    "    for _ in range(sample_size):\n",
    "        yield S.copy()    \n",
    "        for _ in range(sample_distance):\n",
    "            _MCMC_step(S)\n",
    "            \n",
    "def inverse_problem_estimate(m_D, cov_D, approximation='MF'):\n",
    "    C_inv = np.linalg.inv(cov_D - np.outer(m_D,m_D)  )\n",
    "    \n",
    "    if approximation == 'MF':\n",
    "        J_estim = np.diag(1 / (1 - m_D ** 2)) - C_inv\n",
    "        np.fill_diagonal(J_estim, 0)\n",
    "        h_estim = np.arctanh(m_D) - np.dot(J_estim, m_D)\n",
    "    elif approximation == 'TAP':\n",
    "        B = np.outer(m_D, m_D)\n",
    "        # J_ij is given by solution of second degree equation. In case leading coefficient B_ij = 0\n",
    "        # set solution to first degree equation one.\n",
    "        J_estim = # FILL\n",
    "        first_deg_mask = B == 0\n",
    "        J_estim[first_deg_mask] = # FILL\n",
    "        h_estim = # FILL\n",
    "        \n",
    "    return h_estim, J_estim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMMENT**:  the quadratic equation from point a) for J estimated via TAP gives two solutions. Which one to choose? One should choose the one that is most similar to the MF one. In practice, this happens to be the one with the + sign.\n",
    "\n",
    "**COMMENT**: choose a burn-in and max t as defined above. Select MCMC samples every $d_{sample}$ to avoid correlations. This is done above with the argument `sample_distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_estimate(M, N, J0):\n",
    "    h, J = sample_instance(N, J0)\n",
    "    \n",
    "    # s_D, m_D empirical estimates of average and correlations from sampled data.\n",
    "    # Run MCMC sampler and estimate them empirically.\n",
    "    m_D, cov_D = np.zeros(N), np.zeros((N, N))\n",
    "    for s in SK_simul(h, J, sample_size=M, sample_distance=10*N, t_eq=100*N):\n",
    "        m_D = m_D + s\n",
    "        cov_D = cov_D + np.outer(s, s)\n",
    "    m_D /= M\n",
    "    cov_D /= M\n",
    "    \n",
    "    # obtain estimates from MF and TAP inverse problem equations\n",
    "    h_mf, J_mf = # FILL\n",
    "    h_tap, J_tap = # FILL\n",
    "    \n",
    "    return h, J, h_mf, J_mf, h_tap, J_tap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 10000 # sample size\n",
    "\n",
    "N_values=[10,20, 50,100]\n",
    "J0_values = [0.1,1.,2.]\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform all experiments and save all results\n",
    "h_estim = dict()\n",
    "J_estim = dict()\n",
    "h_real = dict()\n",
    "J_real = dict()\n",
    "for J0 in J0_values:\n",
    "    for N in N_values:\n",
    "        print(f'Running sampling and estimation for J0 = {J0}, N = {N}')\n",
    "        h, J, h_mf, J_mf, h_tap, J_tap = # FILL\n",
    "        h_real[(J0, N)], J_real[(J0, N)] = # FILL\n",
    "        h_estim[(J0, N, 'MF')], J_estim[(J0, N, 'MF')] = # FILL\n",
    "        h_estim[(J0, N, 'TAP')], J_estim[(J0, N, 'TAP')] = # FILL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "markers=['o','s','*','D','x','v','>','<']\n",
    "\n",
    "for J0 in J0_values:\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "    c=0 # for choosing the marker\n",
    "    for N in N_values:\n",
    "        \n",
    "        ax[0, 0].scatter(h_real[(J0, N)], h_estim[(J0, N, 'MF')],marker=markers[c],label=f'N={N}')\n",
    "        ax[0, 0].set_title(f'MF vs TRUE h value')\n",
    "        ax[0, 0].set_ylabel('$h_{model}$')\n",
    "        ax[0, 0].legend()\n",
    "        ax[0, 1].scatter(h_real[(J0, N)], h_estim[(J0, N, 'TAP')],marker=markers[c+1],label=f'N={N}')\n",
    "        ax[0, 1].set_title(f'TAP vs TRUE h value')\n",
    "        ax[0, 1].legend()\n",
    "        \n",
    "        mask = np.ones(J_real[(J0, N)].shape, dtype=bool) # useful for neglecting the diagonal\n",
    "        np.fill_diagonal(mask, 0)\n",
    "        ax[1, 0].scatter(J_real[(J0, N)][mask], J_estim[(J0, N, 'MF')][mask],marker=markers[c],label=f'N={N}')\n",
    "        ax[1, 0].set_title(f'MF vs TRUE J value')\n",
    "        ax[1, 0].set_ylabel('$J_{model}$')\n",
    "        ax[1, 0].legend()\n",
    "        ax[1, 1].scatter(J_real[(J0, N)][mask], J_estim[(J0, N, 'TAP')][mask],marker=markers[c+1],label=f'N={N}')\n",
    "        ax[1, 1].set_title(f'TAP vs TRUE J value')\n",
    "        ax[1, 1].legend()\n",
    "        \n",
    "        c+=2\n",
    "        # plot y = x line \n",
    "        for i in (0, 1):\n",
    "            for j in (0, 1):\n",
    "                x = np.linspace(*ax[i, j].get_xlim())\n",
    "                ax[i, j].plot(x, x, '--')\n",
    "        fig.suptitle(f'$J_0$ = {J0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points e) and f)\n",
    " -  Calculate RMSE between inferred and ground-truth values of the parameters.\n",
    " - Generate plots with RMSE as a function of $N$ for the 3 different $J_{0}$ values (using different markers' type). We have a total of 2 plots, one per parameter $J$ or $h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(real, estim,flag_mask=False):\n",
    "    if flag_mask is False:\n",
    "        return np.sqrt( np.sum((real.flatten() - estim.flatten()) ** 2))\n",
    "    else:\n",
    "        mask = np.ones(real.shape, dtype=bool) # useful for neglecting the diagonal\n",
    "        np.fill_diagonal(mask, 0)\n",
    "        return np.sqrt( np.sum((real[mask].flatten() - estim[mask].flatten()) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers=['o','s','*','D','x','v']\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Plot for h\n",
    "'''\n",
    "plt.figure()\n",
    "c=0\n",
    "for J0 in J0_values:  \n",
    "    mf_h_rsme = [rmse(h_real[(J0, N)], h_estim[(J0, N, 'MF')]) for N in N_values]\n",
    "    tap_h_rsme =# FILL\n",
    "    \n",
    "    plt.plot(N_values, mf_h_rsme,color='b', linestyle=':',label=r'MF $J_0$='+str(J0),marker=markers[c])\n",
    "    plt.plot(N_values, tap_h_rsme,color='r', linestyle=':',label=r'TAP $J_0$='+str(J0),marker=markers[c+1])\n",
    "    \n",
    "    c+=1\n",
    "plt.legend()  \n",
    "plt.xlabel('N')\n",
    "plt.ylabel(r'$RMSE_{GT,model}(h)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plot for J\n",
    "'''\n",
    "plt.figure()\n",
    "c=0\n",
    "for J0 in J0_values:  \n",
    "    mf_J_rsme = [rmse(J_real[(J0, N)], J_estim[(J0, N, 'MF')],flag_mask=True) for N in N_values]\n",
    "    tap_J_rsme = # FIll\n",
    "    \n",
    "    plt.plot(N_values, mf_J_rsme,color='b', linestyle=':',label=r'MF $J_0$='+str(J0),marker=markers[c])\n",
    "    plt.plot(N_values, tap_J_rsme,color='r', linestyle=':',label=r'TAP $J_0$='+str(J0),marker=markers[c+1])\n",
    "    \n",
    "    c+=1\n",
    "plt.legend()  \n",
    "plt.xlabel('N')\n",
    "plt.ylabel(r'$RMSE_{GT,model}(J)$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point g)\n",
    " - Plot $\\bar{J}$ and $\\bar{h}$ as a function of $N$, where $\\bar{J}$ is the average over the $i,j$. Similar for $\\bar{h}$. In this plot there should be 3 different curves: one for the ground-truth, one for MF and one for the TAP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(J0_values), 2, figsize=(15, 10), sharex=True)\n",
    "for i, J0 in enumerate(J0_values):\n",
    "    mf_h = [np.mean(h_estim[(J0, N, 'MF')]) for N in N_values]\n",
    "    mf_J = [np.mean(J_estim[(J0, N, 'MF')]) for N in N_values]\n",
    "    tap_h = # FILL\n",
    "    tap_J = # FILL\n",
    "    real_h = # FILL\n",
    "    real_J = # FILL\n",
    "    axes[i, 0].plot(N_values, real_h, 'o-', N_values, mf_h, 'x-', N_values, tap_h, 'v-')\n",
    "    axes[i, 0].set_ylabel(r'mean $h$')\n",
    "    axes[i, 1].plot(N_values, real_J, 'o-', N_values, mf_J, 'x-', N_values, tap_J, 'v-')\n",
    "    axes[i, 1].set_ylabel(r'mean $J$')\n",
    "    axes[i, 1].legend(['real', 'MF', 'TAP'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point h)\n",
    "- Same plot but for the variances $ \\bar{h^{2}}-\\bar{h}^{2}$ and $ \\bar{J^{2}}-\\bar{J}^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(J0_values), figsize=(10, 10), sharex=True)\n",
    "for i, J0 in enumerate(J0_values):\n",
    "    mf_h_var = # FILL\n",
    "    tap_h_var = # FILL\n",
    "    real_h_var = # FILL\n",
    "    axes[i].plot(N_values, real_h_var, 'o:g', N_values, mf_h_var, 'x:b', N_values, tap_h_var, 'v:r')\n",
    "    axes[i].set_ylabel(r'$Var(\\, h\\, )$')\n",
    "    axes[i].legend(['real', 'MF', 'TAP'], loc='upper right')\n",
    "axes[-1].set_xlabel('N')\n",
    "_ = fig.suptitle('Variance of h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(J0_values), figsize=(10, 10), sharex=True)\n",
    "for i, J0 in enumerate(J0_values):\n",
    "    mf_J_var = # FILL\n",
    "    tap_J_var = # FILL\n",
    "    real_J_var = # FILL\n",
    "    axes[i].plot(N_values, real_J_var, 'o-', N_values, mf_J_var, 'x-', N_values, tap_J_var, 'v-')\n",
    "    axes[i].set_ylabel(r'$Var(\\, J\\, )$')\n",
    "    axes[i].legend(['real', 'MF', 'TAP'], loc='upper right')\n",
    "axes[-1].set_xlabel('N')\n",
    "_ = fig.suptitle('Variance of J')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
